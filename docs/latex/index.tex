Toolbox f\+Or Stratified Convective Atmospheres\hypertarget{index_autotoc_md0}{}\doxysection{Executables}\label{index_autotoc_md0}
\begin{DoxyVerb}- tosca    : transient solver for stratified incompressible flows. Temperature stratification is accounted via Boussinesq approximation.
- windToPW : post processor for ParaView visualization. Writes data in XMF/HDF format.
\end{DoxyVerb}
\hypertarget{index_autotoc_md1}{}\doxysection{Boundary Conditions}\label{index_autotoc_md1}
\begin{DoxyVerb}- noSlip               : on all patches
- slip                 : on all patches
- zeroGradient         : on all patches
- fixedGradient        : on all patches (only T equation)
- inflowFunction       : only on kLeft patch. Different types of inflow can be prescribed: ABL inflow using teorethical laws, mapped inflow from a precursor
                         simulation that can be also periodized and/or interpolated if meshes are not consistent.
- velocityWallFunction : shear stress model (only U equation). Can be prescribed on jRight and jLeft patches
- thetaWallFunction    : potential temperature wall model (only T equation). Can be prescribed on jRight and jLeft patches
\end{DoxyVerb}
\hypertarget{index_autotoc_md2}{}\doxysection{Wall Models}\label{index_autotoc_md2}
\begin{DoxyVerb}- Shumann    : applies wall shear stress in the momentum equation according to similarity theory of Paulson and Obuhkov.
               Velocity BC is dependent (velocityWallFunction/thetaWallFunction, type -3).
- Cabot      : prescribes velocity BC according to Cabot formulation (velocityWallFunction, type -1).
\end{DoxyVerb}
\hypertarget{index_autotoc_md3}{}\doxysection{Turbulence Models}\label{index_autotoc_md3}
\begin{DoxyVerb}- Smagorinsky standard model (les 1).
- Dynamic Smagorinsky LES turbulence model with cubic (les 3) or lagrangian (les 4) averaging.
\end{DoxyVerb}
\hypertarget{index_autotoc_md4}{}\doxysection{Turbine Models}\label{index_autotoc_md4}
\begin{DoxyVerb}- UADM : base actuator disk model with imposed Ct and yaw controller
- ADM  : advanced actuator disk model with rotor dynamics and yaw/pitch/rotation controllers
\end{DoxyVerb}
\hypertarget{index_autotoc_md5}{}\doxysection{Acquisition System}\label{index_autotoc_md5}
\begin{DoxyVerb}- ABL wall-parallel planes-averaged statistics as a function of time
- field averages
- domain sections
- probes with advanced parallel I/O writing
- turbine data with advanced parallel I/O writing
\end{DoxyVerb}
\hypertarget{index_autotoc_md6}{}\doxysection{Future Implementations\+:}\label{index_autotoc_md6}
\begin{DoxyVerb}- IBM pressure and viscous forces computation
\end{DoxyVerb}
\hypertarget{index_autotoc_md7}{}\doxysection{Notes\+:}\label{index_autotoc_md7}
\begin{DoxyVerb}- test cases which contain an empty 'inflowDatabase' file require the inflow database. A sample database can be downloaded
  at https://drive.google.com/file/d/17F5wtI5Jc1XGh8crmOVJYVXabC8iQXq1/view?usp=sharing, simply substitute the 'inflowDatabase'
  file with the downloaded folder.
\end{DoxyVerb}
\hypertarget{index_autotoc_md8}{}\doxysection{Installation\+:}\label{index_autotoc_md8}
In order to be installed, T\+O\+S\+CA requires a working C/\+C++ compiler, P\+E\+T\+Sc (version 3.\+14.\+x, 3.\+15.\+x), Open M\+PI (version 4.\+0.\+x, 4.\+1.\+x), H\+D\+F5 and H\+Y\+P\+RE (needed by P\+E\+T\+Ss in order to build some of the matrix solvers we use). T\+O\+S\+CA has been tested with the above version combinations, it could work with other combinations or versions but it has not been tested (especially older versions). We recommend the following versions of the above libraries\+:
\begin{DoxyItemize}
\item gcc \+: 9.\+2.\+0 (\href{https://gcc.gnu.org/}{\texttt{ https\+://gcc.\+gnu.\+org/}})
\item P\+E\+T\+Sc \+: 3.\+15.\+5 (\href{https://ftp.mcs.anl.gov/pub/petsc/}{\texttt{ https\+://ftp.\+mcs.\+anl.\+gov/pub/petsc/}})
\item Open M\+PI \+: 4.\+1.\+2 (\href{https://www.open-mpi.org/software/ompi/v4.1/}{\texttt{ https\+://www.\+open-\/mpi.\+org/software/ompi/v4.\+1/}})
\item H\+Y\+P\+RE \+: 2.\+20.\+0 (\href{https://github.com/hypre-space/hypre/tree/hypre_petsc}{\texttt{ https\+://github.\+com/hypre-\/space/hypre/tree/hypre\+\_\+petsc}}) (check version in /src/\+C\+Make\+Lists.txt)
\item H\+D\+F5 \+: 1.\+12.\+1 (\href{https://www.hdfgroup.org/downloads/hdf5/}{\texttt{ https\+://www.\+hdfgroup.\+org/downloads/hdf5/}}) We suggest to create a folder named \textquotesingle{}software\textquotesingle{}, where the P\+E\+T\+Sc, H\+Y\+P\+RE and T\+O\+S\+CA folders will be located.
\end{DoxyItemize}

Prior to install T\+O\+S\+CA, we suggest creating a folder named {\ttfamily Software} inside \$\+H\+O\+ME, where all the following steps will be performed. In order to compile T\+O\+S\+CA on your system, please follow these steps\+: \begin{DoxyVerb}0. Check your compiler version with `gcc --version`

1. Download PETSc into `$HOME/Software/`

2. Download HYPRE `$HOME/Software/`

3. Download Open MPI: you can download the binaries or compile from source (the latter is recommended if use `environment-modules`).
   If you have only one version of Open MPI installed on your system in the `/usr` directory (using sudo for example), you can omit the
   `--with-mpi-dir='your--path--to--mpicc'` at point 4: Open MPI will be found by the 'ld' library locator.

4. Configure PETSc (will automatically compile HYPRE). We suggest the following configure options:
   `./configure --with-fc=0 --download-f2cblaslapack --with-mpi-dir='your--path--to--mpicc' --download-hypre='your--path--to--hypre \
   --with-64-bit-indices=1 --with-debugging=0`

5. Make PETSc with `make all`

6. Test PETSc with `make check`

7. Save an alias that will tell TOSCA where PETSc is installed in your .bashrc:
   `echo "export PETSC_DIR=$HOME/your--path--to--petsh" >> $HOME/.bashrc`

8. Reload the environment with `source $HOME/.bashrc`

9. Go inside TOSCA/src directory and compile the executables with `make tosca` and `make windToPW`

10. Test the installation by copying `tosca` and `windToPW` in one of the example cases and run the simulation
    and the post-processing with `./tosca` and `./windToPW` respectively. To run in parallel you have to use
    `mpirun -np 'your-number-of-processors' ./tosca`
\end{DoxyVerb}


Credits and Copyright\+: Sebastiano Stipa -\/ Arjun Ajay -\/ Mohammad Hadi -\/ The University of British Columbia 